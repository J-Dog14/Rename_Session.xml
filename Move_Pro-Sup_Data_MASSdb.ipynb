{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-16T19:30:05.437818Z",
     "start_time": "2025-01-16T19:29:59.260839Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import os\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "\n",
    "# Ask user to select folder containing the Session XML file\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "selected_folder = filedialog.askdirectory(initialdir='D:/Pro-Sup Test/Data/')\n",
    "if not selected_folder:\n",
    "    raise ValueError(\"No folder was selected.\")\n",
    "\n",
    "# Extract the test_date from the selected folder name\n",
    "folder_name = os.path.basename(selected_folder)\n",
    "test_date = folder_name.split('_', 1)[0]  # Extract '2024-08-13' from '2024-08-13_105_Growth Plate_'\n",
    "\n",
    "# Find the XML file titled \"Session\" in the selected folder\n",
    "xml_file_path = ''\n",
    "for r, dirs, files in os.walk(selected_folder):\n",
    "    for file in files:\n",
    "        if file.lower().startswith('session') and file.lower().endswith('.xml'):\n",
    "            xml_file_path = os.path.join(r, file)\n",
    "            break\n",
    "    if xml_file_path:\n",
    "        break\n",
    "\n",
    "if not xml_file_path:\n",
    "    raise FileNotFoundError(\"No 'Session' XML file found in the selected folder.\")\n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse(xml_file_path)\n",
    "root_xml = tree.getroot()\n",
    "\n",
    "# Extract required fields from XML\n",
    "name = root_xml.find(\".//Name\").text\n",
    "dob = root_xml.find(\".//DOB\").text\n",
    "height = root_xml.find(\".//Height\").text\n",
    "weight = root_xml.find(\".//Weight\").text\n",
    "injury_history = root_xml.find(\".//Injury_History\").text\n",
    "season_phase = root_xml.find(\".//Season_Phase\").text\n",
    "dynomometer_score = root_xml.find(\".//Dynomometer_Score\").text\n",
    "comments = root_xml.find(\".//Comments\").text\n",
    "\n",
    "# Calculate age from DOB\n",
    "dob_date = datetime.strptime(dob, \"%Y-%m-%d\")\n",
    "today = datetime.today()\n",
    "age = today.year - dob_date.year - ((today.month, today.day) < (dob_date.month, dob_date.day))\n",
    "\n",
    "# Define final columns\n",
    "final_columns = [\n",
    "    'name', 'test_date', 'age', 'height', 'weight', 'injury_history', 'season_phase', 'dynomometer_score', 'comments',\n",
    "    'forearm_rom_0to10', 'forearm_rom_10to20', 'forearm_rom_20to30', 'forearm_rom',\n",
    "    'tot_rom_0to10', 'tot_rom_10to20', 'tot_rom_20to30', 'tot_rom',\n",
    "    'num_of_flips_0_10', 'num_of_flips_10_20', 'num_of_flips_20_30', 'num_of_flips',\n",
    "    'avg_velo_0_10', 'avg_velo_10_20', 'avg_velo_20_30', 'avg_velo'\n",
    "]\n",
    "\n",
    "# Create a DataFrame with XML data and NULL for ASCII columns\n",
    "data = {\n",
    "    'name': name,\n",
    "    'test_date': test_date,\n",
    "    'age': age,\n",
    "    'height': height,\n",
    "    'weight': weight,\n",
    "    'injury_history': injury_history,\n",
    "    'season_phase': season_phase,\n",
    "    'dynomometer_score': dynomometer_score,\n",
    "    'comments': comments,\n",
    "    'forearm_rom_0to10': None,\n",
    "    'forearm_rom_10to20': None,\n",
    "    'forearm_rom_20to30': None,\n",
    "    'forearm_rom': None,\n",
    "    'tot_rom_0to10': None,\n",
    "    'tot_rom_10to20': None,\n",
    "    'tot_rom_20to30': None,\n",
    "    'tot_rom': None,\n",
    "    'num_of_flips_0_10': None,\n",
    "    'num_of_flips_10_20': None,\n",
    "    'num_of_flips_20_30': None,\n",
    "    'num_of_flips': None,\n",
    "    'avg_velo_0_10': None,\n",
    "    'avg_velo_10_20': None,\n",
    "    'avg_velo_20_30': None,\n",
    "    'avg_velo': None\n",
    "}\n",
    "\n",
    "df = pd.DataFrame([data], columns=final_columns)\n",
    "\n",
    "# Connect to the SQLite database and create table if needed\n",
    "db_path = 'D:/Pro-Sup Test/pro-sup_data.sqlite'\n",
    "conn = sqlite3.connect(db_path)\n",
    "\n",
    "# Ensure table exists\n",
    "df.head(0).to_sql('pro_sup_data', conn, if_exists='append', index=False)\n",
    "\n",
    "# Insert the row with XML data and NULL ASCII columns\n",
    "df.to_sql('pro_sup_data', conn, if_exists='append', index=False)\n",
    "\n",
    "conn.close()\n",
    "\n",
    "# In the cell that pulls the XML/ASCII\n",
    "global NEW_NAME, NEW_TEST_DATE\n",
    "NEW_NAME = root_xml.find(\".//Name\").text\n",
    "NEW_TEST_DATE = folder_name.split('_', 1)[0]\n",
    "\n",
    "print(\"Globals set:\", NEW_NAME, NEW_TEST_DATE)\n",
    "\n",
    "print(f\"XML data inserted for {name} with test date {test_date}.\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Globals set: Lee, David 2025-01-16\n",
      "XML data inserted for Lee, David with test date 2025-01-16.\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:30:05.482625Z",
     "start_time": "2025-01-16T19:30:05.438829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "def extract_test_date_from_ascii(ascii_file_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the test date in 'YYYY-MM-DD' format from the first file path in the ASCII file.\n",
    "    \n",
    "    Args:\n",
    "        ascii_file_path (str): Path to the ASCII file.\n",
    "    \n",
    "    Returns:\n",
    "        str: Extracted test date in 'YYYY-MM-DD' format.\n",
    "    \"\"\"\n",
    "    with open(ascii_file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        # Extract the first file path\n",
    "        first_file_path = lines[0].strip().split('\\t')[0]\n",
    "        # Split the path to navigate the folder structure\n",
    "        parts = first_file_path.split('\\\\')  # Split by folder structure\n",
    "        if len(parts) > 4:  # Ensure we have enough subfolders\n",
    "            date_folder = parts[4]  # Get the folder containing the date\n",
    "            # Use regex to extract 'YYYY-MM-DD' format\n",
    "            match = re.match(r'^\\d{4}-\\d{2}-\\d{2}', date_folder)\n",
    "            if match:\n",
    "                return match.group(0)  # Return the matched date\n",
    "            else:\n",
    "                raise ValueError(f\"Unable to extract test date from folder: {date_folder}\")\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected file path structure: Unable to extract test date.\")\n",
    "\n",
    "\n",
    "# Path to ASCII file\n",
    "text_file_path = 'D:/Pro-Sup Test/pro-sup_data.txt'\n",
    "\n",
    "# Dynamically extract the test date from the ASCII file\n",
    "test_date = extract_test_date_from_ascii(text_file_path)\n",
    "print(f\"Extracted test date: {test_date}\")\n",
    "\n",
    "with open(text_file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    data = [line.strip().split('\\t') for line in lines]\n",
    "\n",
    "# Extract header and data rows\n",
    "header = data[1]\n",
    "df_data = [row[1:] for row in data[5:]]\n",
    "df_ascii = pd.DataFrame(df_data, columns=header)\n",
    "\n",
    "# Replace hyphens with underscores\n",
    "df_ascii.columns = [col.replace('-', '_') for col in df_ascii.columns]\n",
    "\n",
    "row_ascii = df_ascii.iloc[0]\n",
    "\n",
    "# Extract ASCII data\n",
    "values = {col: row_ascii.get(col) for col in df_ascii.columns}\n",
    "\n",
    "# Construct the UPDATE statement\n",
    "set_clause = \", \".join([f\"{col} = ?\" for col in values.keys()])\n",
    "params = list(values.values()) + [name, test_date]\n",
    "\n",
    "update_sql = f\"UPDATE pro_sup_data SET {set_clause} WHERE name = ? AND test_date = ?;\"\n",
    "\n",
    "conn = sqlite3.connect('D:/Pro-Sup Test/pro-sup_data.sqlite')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(update_sql, params)\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(f\"ASCII data updated for {name} on test date {test_date}.\")\n",
    "\n",
    "ascii_file_path = 'D:/Pro-Sup Test/pro-sup_data.txt'\n",
    "test_date = extract_test_date_from_ascii(ascii_file_path)\n",
    "print(f\"Extracted test date: {test_date}\")\n"
   ],
   "id": "6774779f4dcc6aaa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted test date: 2025-01-16\n",
      "ASCII data updated for Lee, David on test date 2025-01-16.\n",
      "Extracted test date: 2025-01-16\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:30:06.225033Z",
     "start_time": "2025-01-16T19:30:05.484539Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import dash\n",
    "from dash import dcc, html\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "# Replace these with the actual name and test_date\n",
    "# for the participant you just inserted.\n",
    "# (Or dynamically pass them in from your other script.)\n",
    "NEW_PARTICIPANT_NAME = \"John Doe\"\n",
    "NEW_PARTICIPANT_TEST_DATE = \"2024-08-13\"\n",
    "\n",
    "# Path to your SQLite database\n",
    "DB_PATH = \"D:/Pro-Sup Test/pro-sup_data.sqlite\"\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "def get_database_df(db_path=DB_PATH):\n",
    "    \"\"\"\n",
    "    Pull all data from the pro_sup_data table.\n",
    "    \"\"\"\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    df = pd.read_sql_query(\"SELECT * FROM pro_sup_data\", conn)\n",
    "    conn.close()\n",
    "    return df\n",
    "\n",
    "def build_comparison_figure(name, test_date):\n",
    "    \"\"\"\n",
    "    Build a Plotly figure comparing the newly inserted participant's\n",
    "    data to the rest of the participants' data, focusing on columns\n",
    "    containing 'tot_rom_', 'forearm_rom_', and 'num_of_flips_'.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read everything from the DB\n",
    "    df = get_database_df(DB_PATH)\n",
    "\n",
    "    # Identify columns of interest\n",
    "    columns_of_interest = [\n",
    "        col for col in df.columns \n",
    "        if (\"tot_rom_\" in col) or (\"forearm_rom_\" in col) or (\"num_of_flips_\" in col)\n",
    "    ]\n",
    "\n",
    "    # Separate the newly inserted row from the rest\n",
    "    df_new = df[(df[\"name\"] == name) & (df[\"test_date\"] == test_date)]\n",
    "    df_others = df[(df[\"name\"] != name) | (df[\"test_date\"] != test_date)]\n",
    "\n",
    "    # If for some reason there is more than one new row, just take the first\n",
    "    if len(df_new) == 0:\n",
    "        raise ValueError(f\"No matching row found for name={name} and test_date={test_date}.\")\n",
    "    if len(df_new) > 1:\n",
    "        df_new = df_new.iloc[[0]]\n",
    "\n",
    "    # Convert columns to numeric (in case they are stored as strings)\n",
    "    for c in columns_of_interest:\n",
    "        df_new[c] = pd.to_numeric(df_new[c], errors='coerce')\n",
    "        df_others[c] = pd.to_numeric(df_others[c], errors='coerce')\n",
    "\n",
    "    # Extract the new participant's values as a list\n",
    "    new_values = [df_new.iloc[0][col] for col in columns_of_interest]\n",
    "\n",
    "    # Compute the mean of all other participants for each column\n",
    "    others_means = [df_others[col].mean() for col in columns_of_interest]\n",
    "\n",
    "    # Build a small DataFrame for plotting\n",
    "    plot_df = pd.DataFrame({\n",
    "        \"Column\": columns_of_interest * 2,\n",
    "        \"Value\": new_values + others_means,\n",
    "        \"Group\": [\"New Participant\"] * len(columns_of_interest) + [\"Others (Avg)\"] * len(columns_of_interest)\n",
    "    })\n",
    "\n",
    "    # Create a grouped bar chart\n",
    "    fig = px.bar(\n",
    "        plot_df, \n",
    "        x=\"Column\", \n",
    "        y=\"Value\", \n",
    "        color=\"Group\", \n",
    "        barmode=\"group\", \n",
    "        title=f\"Comparison for {name} ({test_date})\"\n",
    "    )\n",
    "    fig.update_layout(xaxis_title=\"Column of Interest\", yaxis_title=\"Value\")\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Build layout\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Pro-Sup Data Comparison Report\"),\n",
    "    dcc.Graph(id=\"comparison-graph\")\n",
    "])\n",
    "\n",
    "# Use a simple callback or just build the figure once\n",
    "@app.callback(\n",
    "    dash.dependencies.Output(\"comparison-graph\", \"figure\"),\n",
    "    [dash.dependencies.Input(\"comparison-graph\", \"id\")]  # Dummy input to trigger once\n",
    ")\n",
    "def update_graph(_):\n",
    "    return build_comparison_figure(\n",
    "        name=NEW_PARTICIPANT_NAME,\n",
    "        test_date=NEW_PARTICIPANT_TEST_DATE\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run in debug mode. Access it at http://127.0.0.1:8052\n",
    "    app.run_server(debug=True)\n"
   ],
   "id": "f147fec42517b986",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1d048bc5250>"
      ],
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:30:06.305778Z",
     "start_time": "2025-01-16T19:30:06.226088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Reorders the database to be in alphabetical order\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "\n",
    "db_path = \"D:/Pro-Sup Test/pro-sup_data.sqlite\" \n",
    "sort_column = \"name\"     \n",
    "\n",
    "def reorder_all_tables(db_path, sort_column):\n",
    "    try:\n",
    "        # Connect to the database\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Fetch all table names in the database\n",
    "        cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "        tables = cursor.fetchall()\n",
    "\n",
    "        for table in tables:\n",
    "            table_name = table[0]\n",
    "\n",
    "            # Skip system tables like sqlite_sequence\n",
    "            if table_name.startswith(\"sqlite_\"):\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing table: {table_name}\")\n",
    "\n",
    "            # Check if the column exists in the current table\n",
    "            cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "            columns = [info[1] for info in cursor.fetchall()]\n",
    "            if sort_column not in columns:\n",
    "                print(f\"Skipping table '{table_name}' - Column '{sort_column}' not found.\")\n",
    "                continue\n",
    "\n",
    "            # Create a new sorted table\n",
    "            temp_table = f\"{table_name}_sorted\"\n",
    "            cursor.execute(f\"CREATE TABLE {temp_table} AS SELECT * FROM {table_name} ORDER BY {sort_column} ASC;\")\n",
    "            \n",
    "            # Drop the old table\n",
    "            cursor.execute(f\"DROP TABLE {table_name};\")\n",
    "            \n",
    "            # Rename the new table to the original name\n",
    "            cursor.execute(f\"ALTER TABLE {temp_table} RENAME TO {table_name};\")\n",
    "            print(f\"Table '{table_name}' reordered successfully.\")\n",
    "\n",
    "        # Commit changes\n",
    "        conn.commit()\n",
    "        print(\"All tables processed.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "reorder_all_tables(db_path, sort_column)\n"
   ],
   "id": "24f6ebe318f5ef04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing table: pro_sup_data\n",
      "Table 'pro_sup_data' reordered successfully.\n",
      "All tables processed.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-16T19:30:06.310752Z",
     "start_time": "2025-01-16T19:30:06.307284Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7ef4ba9542b861ab",
   "outputs": [],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
